{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sree Ganeshaaya Namaha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competion - Santander Customer Transaction Prediction\n",
    "https://www.kaggle.com/c/santander-customer-transaction-prediction\n",
    "\n",
    "This Kernel got 0.74 in the Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.simplefilter(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the timestamp along with the log message\n",
    "def myprint(*argv):\n",
    "    sstr=datetime.now()\n",
    "    for arg in argv:\n",
    "        sstr=\"{} {}\".format(sstr,arg)\n",
    "    print(sstr)\n",
    "\n",
    "starttime=time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 20:28:40.600583 Columns:\n",
      "2019-04-16 20:28:40.600839  ID_code target var_0 var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 var_26 var_27 var_28 var_29 var_30 var_31 var_32 var_33 var_34 var_35 var_36 var_37 var_38 var_39 var_40 var_41 var_42 var_43 var_44 var_45 var_46 var_47 var_48 var_49 var_50 var_51 var_52 var_53 var_54 var_55 var_56 var_57 var_58 var_59 var_60 var_61 var_62 var_63 var_64 var_65 var_66 var_67 var_68 var_69 var_70 var_71 var_72 var_73 var_74 var_75 var_76 var_77 var_78 var_79 var_80 var_81 var_82 var_83 var_84 var_85 var_86 var_87 var_88 var_89 var_90 var_91 var_92 var_93 var_94 var_95 var_96 var_97 var_98 var_99 var_100 var_101 var_102 var_103 var_104 var_105 var_106 var_107 var_108 var_109 var_110 var_111 var_112 var_113 var_114 var_115 var_116 var_117 var_118 var_119 var_120 var_121 var_122 var_123 var_124 var_125 var_126 var_127 var_128 var_129 var_130 var_131 var_132 var_133 var_134 var_135 var_136 var_137 var_138 var_139 var_140 var_141 var_142 var_143 var_144 var_145 var_146 var_147 var_148 var_149 var_150 var_151 var_152 var_153 var_154 var_155 var_156 var_157 var_158 var_159 var_160 var_161 var_162 var_163 var_164 var_165 var_166 var_167 var_168 var_169 var_170 var_171 var_172 var_173 var_174 var_175 var_176 var_177 var_178 var_179 var_180 var_181 var_182 var_183 var_184 var_185 var_186 var_187 var_188 var_189 var_190 var_191 var_192 var_193 var_194 var_195 var_196 var_197 var_198 var_199\n",
      "2019-04-16 20:28:40.606572 First Row:\n",
      "2019-04-16 20:28:40.607565 \n",
      "ID_code:train_0\n",
      "target:0\n",
      "var_0:8.9255\n",
      "var_1:-6.7863\n",
      "var_2:11.9081\n",
      "var_3:5.093\n",
      "var_4:11.4607\n",
      "var_5:-9.2834\n",
      "var_6:5.1187\n",
      "var_7:18.6266\n",
      "var_8:-4.92\n",
      "var_9:5.747000000000001\n",
      "var_10:2.9252\n",
      "var_11:3.1821\n",
      "var_12:14.0137\n",
      "var_13:0.5745\n",
      "var_14:8.7989\n",
      "var_15:14.5691\n",
      "var_16:5.7487\n",
      "var_17:-7.2393\n",
      "var_18:4.284\n",
      "var_19:30.7133\n",
      "var_20:10.535\n",
      "var_21:16.2191\n",
      "var_22:2.5791\n",
      "var_23:2.4716\n",
      "var_24:14.3831\n",
      "var_25:13.4325\n",
      "var_26:-5.1488\n",
      "var_27:-0.4073\n",
      "var_28:4.9306\n",
      "var_29:5.9965\n",
      "var_30:-0.3085\n",
      "var_31:12.9041\n",
      "var_32:-3.8766\n",
      "var_33:16.8911\n",
      "var_34:11.192\n",
      "var_35:10.5785\n",
      "var_36:0.6764\n",
      "var_37:7.8871\n",
      "var_38:4.6667\n",
      "var_39:3.8743\n",
      "var_40:-5.2387\n",
      "var_41:7.3746\n",
      "var_42:11.5767\n",
      "var_43:12.0446\n",
      "var_44:11.6418\n",
      "var_45:-7.017\n",
      "var_46:5.9226\n",
      "var_47:-14.2136\n",
      "var_48:16.0283\n",
      "var_49:5.3253\n",
      "var_50:12.9194\n",
      "var_51:29.046\n",
      "var_52:-0.6940000000000001\n",
      "var_53:5.1736\n",
      "var_54:-0.7474\n",
      "var_55:14.8322\n",
      "var_56:11.2668\n",
      "var_57:5.3822\n",
      "var_58:2.0183\n",
      "var_59:10.1166\n",
      "var_60:16.1828\n",
      "var_61:4.959\n",
      "var_62:2.0771\n",
      "var_63:-0.2154\n",
      "var_64:8.6748\n",
      "var_65:9.5319\n",
      "var_66:5.8056\n",
      "var_67:22.4321\n",
      "var_68:5.0109\n",
      "var_69:-4.7010000000000005\n",
      "var_70:21.6374\n",
      "var_71:0.5663\n",
      "var_72:5.1999\n",
      "var_73:8.86\n",
      "var_74:43.1127\n",
      "var_75:18.3816\n",
      "var_76:-2.344\n",
      "var_77:23.4104\n",
      "var_78:6.5199\n",
      "var_79:12.1983\n",
      "var_80:13.6468\n",
      "var_81:13.8372\n",
      "var_82:1.3675\n",
      "var_83:2.9423\n",
      "var_84:-4.5213\n",
      "var_85:21.4669\n",
      "var_86:9.3225\n",
      "var_87:16.4597\n",
      "var_88:7.9984\n",
      "var_89:-1.7069\n",
      "var_90:-21.4494\n",
      "var_91:6.7806\n",
      "var_92:11.0924\n",
      "var_93:9.9913\n",
      "var_94:14.8421\n",
      "var_95:0.1812\n",
      "var_96:8.9642\n",
      "var_97:16.2572\n",
      "var_98:2.1743\n",
      "var_99:-3.4132\n",
      "var_100:9.4763\n",
      "var_101:13.3102\n",
      "var_102:26.5376\n",
      "var_103:1.4403\n",
      "var_104:14.71\n",
      "var_105:6.0454\n",
      "var_106:9.5426\n",
      "var_107:17.1554\n",
      "var_108:14.1104\n",
      "var_109:24.3627\n",
      "var_110:2.0323\n",
      "var_111:6.7602\n",
      "var_112:3.9141\n",
      "var_113:-0.4851\n",
      "var_114:2.524\n",
      "var_115:1.5093\n",
      "var_116:2.5516\n",
      "var_117:15.5752\n",
      "var_118:-13.4221\n",
      "var_119:7.2739\n",
      "var_120:16.0094\n",
      "var_121:9.7268\n",
      "var_122:0.8897\n",
      "var_123:0.7754\n",
      "var_124:4.2218\n",
      "var_125:12.0039\n",
      "var_126:13.8571\n",
      "var_127:-0.7338\n",
      "var_128:-1.9245\n",
      "var_129:15.4462\n",
      "var_130:12.8287\n",
      "var_131:0.3587\n",
      "var_132:9.6508\n",
      "var_133:6.5674\n",
      "var_134:5.1726\n",
      "var_135:3.1345\n",
      "var_136:29.4547\n",
      "var_137:31.4045\n",
      "var_138:2.8279\n",
      "var_139:15.6599\n",
      "var_140:8.3307\n",
      "var_141:-5.6011\n",
      "var_142:19.0614\n",
      "var_143:11.2663\n",
      "var_144:8.6989\n",
      "var_145:8.3694\n",
      "var_146:11.5659\n",
      "var_147:-16.4727\n",
      "var_148:4.0288\n",
      "var_149:17.9244\n",
      "var_150:18.5177\n",
      "var_151:10.78\n",
      "var_152:9.0056\n",
      "var_153:16.6964\n",
      "var_154:10.4838\n",
      "var_155:1.6573\n",
      "var_156:12.1749\n",
      "var_157:-13.1324\n",
      "var_158:17.6054\n",
      "var_159:11.5423\n",
      "var_160:15.4576\n",
      "var_161:5.3133\n",
      "var_162:3.6159\n",
      "var_163:5.0384\n",
      "var_164:6.676\n",
      "var_165:12.6644\n",
      "var_166:2.7004\n",
      "var_167:-0.6975\n",
      "var_168:9.5981\n",
      "var_169:5.4879\n",
      "var_170:-4.7645\n",
      "var_171:-8.4254\n",
      "var_172:20.8773\n",
      "var_173:3.1531\n",
      "var_174:18.5618\n",
      "var_175:7.7423\n",
      "var_176:-10.1245\n",
      "var_177:13.7241\n",
      "var_178:-3.5189\n",
      "var_179:1.7202\n",
      "var_180:-8.4051\n",
      "var_181:9.0164\n",
      "var_182:3.0657\n",
      "var_183:14.3691\n",
      "var_184:25.8398\n",
      "var_185:5.8764\n",
      "var_186:11.8411\n",
      "var_187:-19.7159\n",
      "var_188:17.5743\n",
      "var_189:0.5857\n",
      "var_190:4.4354\n",
      "var_191:3.9642\n",
      "var_192:3.1364\n",
      "var_193:1.6909999999999998\n",
      "var_194:18.5227\n",
      "var_195:-2.3978\n",
      "var_196:7.8784\n",
      "var_197:8.5635\n",
      "var_198:12.7803\n",
      "var_199:-1.0914\n",
      "2019-04-16 20:28:42.536190 target_arr= (200000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1=pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "cols=df1.columns\n",
    "scols=\"\"\n",
    "for col in cols:\n",
    "    scols=\"{} {}\".format(scols,col)\n",
    "myprint(\"Columns:\")\n",
    "myprint(scols)\n",
    "\n",
    "row1=df1.iloc[0]\n",
    "srow1=\"\"\n",
    "for col in cols:\n",
    "    srow1=\"{}\\n{}:{}\".format(srow1, col, row1[col])\n",
    "myprint(\"First Row:\")\n",
    "myprint(srow1)\n",
    "\n",
    "arr1=df1.values#convert the dataframe to list\n",
    "target_arr=arr1[:,1]#second column is target\n",
    "train_params=arr1[:,2:]\n",
    "\n",
    "myprint(\"target_arr=\",target_arr.shape)\n",
    "data_arr=[]\n",
    "for i,params in enumerate(train_params):\n",
    "    data_arr.append((target_arr[i],params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 20:29:23.587067 train_params= (200000, 200)\n",
      "2019-04-16 20:29:23.588033 test_params= (200000, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "df_test=pd.read_csv(\"../input/test.csv\")\n",
    "arrtmp=df_test.values#convert the dataframe to list\n",
    "test_codes=arrtmp[:,0]#first column is the code.\n",
    "test_params=arrtmp[:,1:]#\n",
    "test_arr=[]\n",
    "for i in range(0,len(test_params)):\n",
    "    test_arr.append((test_codes[i],test_params[i]))\n",
    "\n",
    "myprint(\"train_params=\",train_params.shape)\n",
    "myprint(\"test_params=\",test_params.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler - fit for train and test params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 20:29:27.416240 all_params= (400000, 200)\n"
     ]
    }
   ],
   "source": [
    "all_params=np.concatenate(((np.array(train_params)),np.array(test_params)), axis=0)\n",
    "myprint(\"all_params=\",all_params.shape)\n",
    "\n",
    "scaler=preprocessing.StandardScaler().fit(all_params)\n",
    "\n",
    "#minmax_scaled_arr=minmax_scaler.transform(param_arr1)\n",
    "#myprint (\"minmax_scaled_arr=\",minmax_scaled_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA - Fit on train data to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cols=160\n",
    "pca = PCA(n_components=pca_cols)\n",
    "pca.fit(scaler.transform(all_params))\n",
    "\n",
    "train_scaled_params=scaler.transform(train_params)\n",
    "train_pca_params=pca.transform(train_scaled_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit LightGBM on the train data and get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds=lgb.Dataset(train_pca_params,target_arr)\n",
    "config_params = {'num_leaves': 13,\n",
    "         'min_data_in_leaf': 80,\n",
    "          'min_sum_hessian_in_leaf': 10.0,\n",
    "         'objective': 'binary',\n",
    "          'boost_from_average': False,\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.0083,\n",
    "         'boost': 'gbdt',\n",
    "         'bagging_freq': 5,\n",
    "         'tree_learner': \"serial\",\n",
    "         'bagging_fraction': 0.333,\n",
    "         'feature_fraction': 0.041,\n",
    "         #'reg_alpha': 1.738,\n",
    "         #'reg_lambda': 4.99,\n",
    "         'metric': 'auc',\n",
    "         #'min_gain_to_split': 0.01077313523861969,\n",
    "         #'min_child_weight': 19.428902804238373,\n",
    "         'num_threads': 8}\n",
    "lgb_model=lgb.train(config_params,ds)\n",
    "feature_importance=lgb_model.feature_importance()\n",
    "dict_f_imp={}\n",
    "for i,val in enumerate(feature_importance):\n",
    "    dict_f_imp[i]=val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider only the features that have importance > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features= [0, 71, 116, 34, 89, 2, 147, 72, 125, 138, 56, 33, 43, 95, 5, 22, 94, 155, 81, 82, 129, 144, 7, 18, 44, 67, 84, 92, 96, 120, 151, 13, 17, 30, 70, 86, 87, 101, 104, 110, 113, 140, 142, 149, 154, 32, 41, 42, 47, 61, 63, 69, 75, 83, 99, 102, 105, 127, 153, 59, 64, 93, 106, 111, 118, 121, 126, 132, 134, 156, 157, 158, 6, 9, 19, 21, 28, 35, 50, 51, 58, 77, 80, 98, 108, 117, 139, 145, 148, 1, 4, 15, 25, 27, 37, 85, 109, 115, 122, 159, 8, 10, 24, 29, 45, 48, 60, 73, 78, 97, 124, 128, 131, 135, 150, 152, 11, 36, 38, 53, 62, 65, 68, 90, 137, 141, 146, 14, 39, 40, 49, 55, 79, 100, 103, 107, 114, 23, 31, 88, 91, 136]\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "sorted_features=sorted(dict_f_imp.items(),key=lambda x: x[1],reverse=True)\n",
    "features=[k for (k,v) in sorted_features if v>1]\n",
    "print(\"features=\",features)\n",
    "num_cols=len(features)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model - contains two LSTM networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_reduced_data_arr(raw_params_arr):\n",
    "    out=[]\n",
    "    for raw_params in raw_params_arr:\n",
    "        d2=[]\n",
    "        for indx in features:\n",
    "            d2.append(raw_params[indx])\n",
    "        out.append(d2)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,scaler,pca,num_cols):\n",
    "        super(Model,self).__init__()\n",
    "        self.scaler=scaler\n",
    "        self.pca=pca\n",
    "        input_size=num_cols\n",
    "        myprint(\"input_size=\",input_size)\n",
    "        self.hidden_size=int(num_cols * 0.1)\n",
    "        self.hidden_size2=int(self.hidden_size * 0.1)\n",
    "        if ( self.hidden_size2 < 10):\n",
    "            self.hidden_size2=10\n",
    "        myprint(\"hidden_size=\",self.hidden_size)\n",
    "        myprint(\"hidden_size2=\",self.hidden_size2)\n",
    "        self.layers=1\n",
    "        \n",
    "        self.lstm=nn.LSTM(input_size,self.hidden_size,self.layers, bidirectional=True, \n",
    "                           dropout=0.1, batch_first=True)\n",
    "        \n",
    "        self.lstm2=nn.LSTM(self.hidden_size*2,self.hidden_size2,self.layers, bidirectional=True, \n",
    "                           dropout=0.1, batch_first=True)\n",
    "        \n",
    "        self.op_classes=2\n",
    "        self.fc=nn.Linear(self.hidden_size2*2,self.op_classes)\n",
    "\n",
    "        self.figcnt=0\n",
    "        \n",
    "    def forward(self,indata,h=None,c=None,h2=None,c2=None):\n",
    "        #apply scaler and pca\n",
    "        indata=self.pca.transform(self.scaler.transform(indata))\n",
    "        indata=get_reduced_data_arr(indata)\n",
    "        indata=torch.Tensor(indata)\n",
    "        indata=indata.unsqueeze(1)\n",
    "        \n",
    "        if h is None:\n",
    "            h = torch.zeros(self.layers*2, indata.shape[0], self.hidden_size)\n",
    "            c = torch.zeros(self.layers*2, indata.shape[0], self.hidden_size)\n",
    "            \n",
    "        _, (h,c)=self.lstm(indata,(h,c))\n",
    "        cat=torch.cat((h[-2,:,:],h[-1,:,:]),dim=1)        \n",
    "        cat=cat.unsqueeze(1)\n",
    "\n",
    "        if h2 is None:\n",
    "            h2 = torch.zeros(self.layers*2, cat.shape[0], self.hidden_size2)\n",
    "            c2 = torch.zeros(self.layers*2, cat.shape[0], self.hidden_size2)\n",
    "        \n",
    "        _, (h2,c2)=self.lstm2(cat,(h2,c2))        \n",
    "        cat2=torch.cat((h2[-2,:,:],h2[-1,:,:]),dim=1)        \n",
    "            \n",
    "        out = self.fc(cat2)\n",
    "        return out, (h,c), (h2,c2)    \n",
    "        \n",
    "    def round_prediction(self,preds_sigmoid):    \n",
    "        #round predictions to the closest integer\n",
    "        #preds=np.array([0 if i<0.7 else 1 for i in preds_sigmoid])\n",
    "        preds=np.array([0 if i>j else 1 for [i,j] in preds_sigmoid])\n",
    "        return preds\n",
    "        \n",
    "    def binary_accuracy(self,rounded_preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        #round predictions to the closest integer\n",
    "        \n",
    "        correct = (torch.Tensor(rounded_preds) == torch.Tensor(y)).float() #convert into float for division     \n",
    "        acc = correct.sum()/len(correct)\n",
    "        return acc, correct.sum().item()        \n",
    "    \n",
    "    def plot_metrics(self,y_pred_rounded,batch_target):                        \n",
    "\n",
    "        # Plot non-normalized confusion matrix\n",
    "        plt.figure(self.figcnt)\n",
    "        self.figcnt+=1\n",
    "        self.plot_confusion(batch_target,y_pred_rounded) \n",
    "\n",
    "        #fpr, tpr, thresholds = roc_curve(batch_target, y_pred_rounded, pos_label=2)\n",
    "        #myprint(\"fpr=\",fpr,\", tpr=\",tpr,\",thresholds=\",thresholds)\n",
    "    \n",
    "    def plot_confusion(self,\n",
    "             batch_target,\n",
    "             pred_rounded,\n",
    "             classes=[0,1],\n",
    "             normalize=False,\n",
    "             title='Confusion matrix',\n",
    "             cmap=plt.cm.Blues):\n",
    "\n",
    "        cm=confusion_matrix(batch_target, pred_rounded)#, labels=class_names)\n",
    "        myprint(cm)\n",
    "\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            myprint(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            myprint('Confusion matrix, without normalization')\n",
    "    \n",
    "        \n",
    "    \n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "    \n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_prediction(self,y_pred_sigmoid,y_pred_rounded):\n",
    "        y=[]\n",
    "        cnt=1\n",
    "        for i in y_pred_sigmoid:\n",
    "            y.append(cnt)\n",
    "            cnt=cnt+1 \n",
    "    \n",
    "        plt.figure(self.figcnt)\n",
    "        plt.title(\"sigmoid_{}\".format(self.figcnt))\n",
    "        #plt.scatter(y,y_pred_sigmoid,s=5)\n",
    "        x=[i for [i,j] in y_pred_sigmoid]\n",
    "        y=[j for [i,j] in y_pred_sigmoid]\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        plt.scatter(x,y,s=5)\n",
    "        plt.show()\n",
    "        self.figcnt+=1\n",
    "                        \n",
    "        #plt.figure(self.figcnt)\n",
    "        #plt.title(\"rounded_{}\".format(self.figcnt))\n",
    "        #plt.scatter(y,y_pred_rounded,s=5)\n",
    "        #plt.show()\n",
    "        #self.figcnt+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsampling the data - as train data has very less number of samples with class=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 20:32:09.649245 total= 200000  class0= 179902 0.89951  class1= 20098 0.10049\n",
      "2019-04-16 20:32:09.649245 diff= 159804\n",
      "2019-04-16 20:32:09.649245 0\n",
      "2019-04-16 20:32:14.812816 50000\n",
      "2019-04-16 20:32:20.980441 100000\n",
      "2019-04-16 20:32:28.528370 150000\n",
      "2019-04-16 20:32:30.177696 done\n",
      "2019-04-16 20:32:30.354297 total= 359804  new_class0= 179902 0.5  new_class1= 179902 0.5\n"
     ]
    }
   ],
   "source": [
    "class_0=[(cls,params) for (cls,params) in data_arr if cls==0]\n",
    "class_1=[(cls,params) for (cls,params) in data_arr if cls==1]\n",
    "total=len(target_arr)\n",
    "num_0=len(class_0)\n",
    "num_1=len(class_1)\n",
    "myprint(\"total=\",total,\" class0=\",num_0, num_0/total, \" class1=\",num_1, num_1/total)\n",
    "\n",
    "\n",
    "# Recreate Training set.\n",
    "diff=num_0-num_1\n",
    "myprint(\"diff=\",diff)\n",
    "\n",
    "j=0\n",
    "for i in range (0,diff):    \n",
    "    if  i % 50000 == 0:\n",
    "        myprint (i)\n",
    "    pos=np.random.randint(0,len(data_arr))\n",
    "    \n",
    "    #insert class1 sample randomly into train_arr.\n",
    "    data_arr.insert(pos,class_1[j])\n",
    "    \n",
    "    j+=1\n",
    "    if j >= len(class_1):\n",
    "        j=0\n",
    "myprint(\"done\")\n",
    "\n",
    "new_class_0=[(cls,params) for (cls,params) in data_arr if cls==0]\n",
    "new_class_1=[(cls,params) for (cls,params) in data_arr if cls==1]\n",
    "new_total=len(data_arr)\n",
    "new_num_0=len(new_class_0)\n",
    "new_num_1=len(new_class_1)\n",
    "myprint(\"total=\",new_total,\" new_class0=\",new_num_0, new_num_0/new_total, \" new_class1=\",new_num_1, new_num_1/new_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Optimizer, Loss and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 20:35:30.220003 input_size= 142\n",
      "2019-04-16 20:35:30.220003 hidden_size= 14\n",
      "2019-04-16 20:35:30.220003 hidden_size2= 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\myprograms\\python\\python35\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model=Model(scaler,pca,num_cols)\n",
    "#optimizer=optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "\n",
    "lr=0.1\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#decay LR by a factor of 0.1 every 2 epochs\n",
    "step_size=2\n",
    "scheduler=lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=0.1)\n",
    "\n",
    "#loss_fn=torch.nn.MSELoss(reduction=\"sum\")\n",
    "#loss_fn=nn.CrossEntropyLoss()#we don't use this here as it is btter suited for multi class classificaiton.\n",
    "\n",
    "#BCEWithLogitsLoss combines the sigmoid layer and the BCELoss in one single class but is numerically more stable and hence, should be preferred.\n",
    "#Note here that you donâ€™t need to pass the input tensor to the sigmoid layer before training with BCEWithLogitsLoss.\n",
    "loss_fn=nn.BCEWithLogitsLoss()#binary cross entropy loss\n",
    "h,c,h2,c2=None,None,None,None\n",
    "\n",
    "batch_size=10000\n",
    "scheduler.step()        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function - Shuffle the data for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,h,c,h2,c2):    \n",
    "    \n",
    "    model.train()\n",
    "    loss_num=1\n",
    "    for epoch in range(0,epochs):\n",
    "        \n",
    "        if scheduler.get_lr()[0] > 0.0001 and loss_num<0.5:\n",
    "            scheduler.step()        \n",
    "        \n",
    "        for i in range (0,10):                    \n",
    "            np.random.shuffle(data_arr)\n",
    "        \n",
    "        \n",
    "        epoch_loss=0\n",
    "        batch=0\n",
    "        total=0\n",
    "        correct=0\n",
    "        cnt=0\n",
    "        \n",
    "        lth=len(data_arr)\n",
    "        for b_start in range(0,lth, batch_size):\n",
    "            b_end=b_start+batch_size\n",
    "            batch+=1\n",
    "            cnt=cnt+1\n",
    "            #handle length of last batch\n",
    "            if b_end > lth:\n",
    "                b_end=lth\n",
    "                b_start=lth-batch_size\n",
    "            b_data=data_arr[b_start:b_end]\n",
    "            b_params=[params for (cls,params) in b_data]\n",
    "            \n",
    "            \n",
    "            #break\n",
    "            b_target=[cls for (cls,params) in b_data]\n",
    "            b_target_2d=[[1,0] if cls==0 else [0,1] for cls in b_target]\n",
    "            b_target_2d=torch.Tensor(b_target_2d)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred,(h,c),(h2,c2)=model(b_params,h,c,h2,c2)\n",
    "            \n",
    "            loss=loss_fn(pred,b_target_2d)\n",
    "            loss_num=loss.item()  \n",
    "\n",
    "            #loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ## detach the gru from the previous history/graph.\n",
    "            ## Otherwise, it will hog the memory.\n",
    "            h.detach_()\n",
    "            c.detach_()\n",
    "            h2.detach_()\n",
    "            c2.detach_()\n",
    "            \n",
    "\n",
    "            \n",
    "            pred_sigmoid=torch.sigmoid(pred)\n",
    "            #print (pred_sigmoid)\n",
    "            pred_round=model.round_prediction(pred_sigmoid.detach().numpy())\n",
    "            #print(pred_round)\n",
    "            #print(b_target)\n",
    "            acc,correct=model.binary_accuracy(pred_round,b_target)\n",
    "\n",
    "            if cnt % 1 == 0:                \n",
    "                myprint(\"acc=\",acc,\"correct predictions=\",correct, \"loss=\",loss_num, \"epoch=\",epoch, \"batch=\",batch,\"lr=\", scheduler.get_lr())\n",
    "                #model.plot_metrics(pred_round,b_target)\n",
    "            nowtime=time.time()\n",
    "            if (nowtime-starttime) >= 330*60:\n",
    "                print(\"TIMEOUT\")\n",
    "                print(\"acc=\",acc,\"correct predictions=\",correct, \"loss=\",loss_num, \"epoch=\",epoch, \"batch=\",batch,\"lr=\", scheduler.get_lr())\n",
    "\n",
    "                return (h,c), (h2,c2)\n",
    "                \n",
    "    myprint(\"acc=\",acc,\"correct predictions=\",correct, \"loss=\",loss_num, \"epoch=\",epoch, \"batch=\",batch,\"lr=\", scheduler.get_lr())\n",
    "    return (h,c), (h2,c2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 20:35:38.103260 acc= 0.5080000162124634 correct predictions= 5080.0 loss= 0.6987212896347046 epoch= 0 batch= 1 lr= [0.1]\n",
      "2019-04-16 20:35:40.570763 acc= 0.5692999958992004 correct predictions= 5693.0 loss= 0.6858659982681274 epoch= 0 batch= 2 lr= [0.1]\n",
      "2019-04-16 20:35:42.987218 acc= 0.614300012588501 correct predictions= 6143.0 loss= 0.6640947461128235 epoch= 0 batch= 3 lr= [0.1]\n",
      "2019-04-16 20:35:45.562483 acc= 0.6543999910354614 correct predictions= 6544.0 loss= 0.6286553740501404 epoch= 0 batch= 4 lr= [0.1]\n",
      "2019-04-16 20:35:48.214657 acc= 0.6970999836921692 correct predictions= 6971.0 loss= 0.5826341509819031 epoch= 0 batch= 5 lr= [0.1]\n",
      "2019-04-16 20:35:50.610032 acc= 0.7141000032424927 correct predictions= 7141.0 loss= 0.5667421817779541 epoch= 0 batch= 6 lr= [0.1]\n",
      "2019-04-16 20:35:53.139798 acc= 0.7190999984741211 correct predictions= 7191.0 loss= 0.5759921669960022 epoch= 0 batch= 7 lr= [0.1]\n",
      "2019-04-16 20:35:55.784837 acc= 0.7419000267982483 correct predictions= 7419.0 loss= 0.5319328308105469 epoch= 0 batch= 8 lr= [0.1]\n",
      "2019-04-16 20:35:58.231151 acc= 0.748199999332428 correct predictions= 7482.0 loss= 0.5329446196556091 epoch= 0 batch= 9 lr= [0.1]\n",
      "2019-04-16 20:36:00.768338 acc= 0.7422000169754028 correct predictions= 7422.0 loss= 0.5374146699905396 epoch= 0 batch= 10 lr= [0.1]\n",
      "2019-04-16 20:36:03.128855 acc= 0.7519000172615051 correct predictions= 7519.0 loss= 0.523773729801178 epoch= 0 batch= 11 lr= [0.1]\n",
      "2019-04-16 20:36:05.660146 acc= 0.757099986076355 correct predictions= 7571.0 loss= 0.5123867392539978 epoch= 0 batch= 12 lr= [0.1]\n",
      "2019-04-16 20:36:08.266971 acc= 0.7588000297546387 correct predictions= 7588.0 loss= 0.5084804892539978 epoch= 0 batch= 13 lr= [0.1]\n",
      "2019-04-16 20:36:10.777754 acc= 0.7585999965667725 correct predictions= 7586.0 loss= 0.5083805918693542 epoch= 0 batch= 14 lr= [0.1]\n",
      "2019-04-16 20:36:13.278723 acc= 0.765999972820282 correct predictions= 7660.0 loss= 0.495742529630661 epoch= 0 batch= 15 lr= [0.1]\n",
      "2019-04-16 20:36:15.768673 acc= 0.7652999758720398 correct predictions= 7653.0 loss= 0.49394461512565613 epoch= 0 batch= 16 lr= [0.1]\n",
      "2019-04-16 20:36:18.363005 acc= 0.7645000219345093 correct predictions= 7645.0 loss= 0.49277183413505554 epoch= 0 batch= 17 lr= [0.1]\n",
      "2019-04-16 20:36:20.798972 acc= 0.76910001039505 correct predictions= 7691.0 loss= 0.4915142059326172 epoch= 0 batch= 18 lr= [0.1]\n",
      "2019-04-16 20:36:23.126453 acc= 0.7728999853134155 correct predictions= 7729.0 loss= 0.4897293448448181 epoch= 0 batch= 19 lr= [0.1]\n",
      "2019-04-16 20:36:25.769014 acc= 0.7718999981880188 correct predictions= 7719.0 loss= 0.48473262786865234 epoch= 0 batch= 20 lr= [0.1]\n",
      "2019-04-16 20:36:28.481595 acc= 0.7796000242233276 correct predictions= 7796.0 loss= 0.4704408347606659 epoch= 0 batch= 21 lr= [0.1]\n",
      "2019-04-16 20:36:31.011917 acc= 0.772599995136261 correct predictions= 7726.0 loss= 0.4792272448539734 epoch= 0 batch= 22 lr= [0.1]\n",
      "2019-04-16 20:36:33.428521 acc= 0.7732999920845032 correct predictions= 7733.0 loss= 0.47657787799835205 epoch= 0 batch= 23 lr= [0.1]\n",
      "2019-04-16 20:36:36.127160 acc= 0.7749000191688538 correct predictions= 7749.0 loss= 0.4779600203037262 epoch= 0 batch= 24 lr= [0.1]\n",
      "2019-04-16 20:36:38.787408 acc= 0.7817000150680542 correct predictions= 7817.0 loss= 0.4699915051460266 epoch= 0 batch= 25 lr= [0.1]\n",
      "2019-04-16 20:36:41.287758 acc= 0.7824000120162964 correct predictions= 7824.0 loss= 0.46513330936431885 epoch= 0 batch= 26 lr= [0.1]\n",
      "2019-04-16 20:36:43.868859 acc= 0.79339998960495 correct predictions= 7934.0 loss= 0.45423877239227295 epoch= 0 batch= 27 lr= [0.1]\n",
      "2019-04-16 20:36:46.323510 acc= 0.7843999862670898 correct predictions= 7844.0 loss= 0.46025195717811584 epoch= 0 batch= 28 lr= [0.1]\n",
      "2019-04-16 20:36:49.076028 acc= 0.7889999747276306 correct predictions= 7890.0 loss= 0.4531087279319763 epoch= 0 batch= 29 lr= [0.1]\n",
      "2019-04-16 20:36:51.548559 acc= 0.7870000004768372 correct predictions= 7870.0 loss= 0.46071356534957886 epoch= 0 batch= 30 lr= [0.1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cdfe8af6b0d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m140\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-3a630bbcb4e8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, h, c, h2, c2)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_target_2d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\myprograms\\python\\python35\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0ffd935ef5af>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, indata, h, c, h2, c2)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m#apply scaler and pca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mindata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mindata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_reduced_data_arr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mindata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\myprograms\\python\\python35\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, y, copy)\u001b[0m\n\u001b[0;32m    770\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n\u001b[0;32m    771\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\myprograms\\python\\python35\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(h,c),(h2,c2)=train(140,h,c,h2,c2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function to write the submission.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(): \n",
    "    myprint(\"TEST STARTED\")\n",
    "    \n",
    "    ffile=open(\"submission.csv\",\"w\")\n",
    "    ffile.write(\"ID_code,target\\n\")\n",
    "    \n",
    "    #set the mode to eval.\n",
    "    model.eval()\n",
    "    \n",
    "    lth=len(test_arr)\n",
    "    \n",
    "    for i in range (0,lth,batch_size):\n",
    "        end=i+batch_size        \n",
    "        \n",
    "        if end > lth:\n",
    "            end=lth\n",
    "            \n",
    "        batch_input=test_params[i:end]                    \n",
    "        batch_id=test_codes[i:end]\n",
    "                            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred,(_,_),(_,_)=model(batch_input)            \n",
    "        \n",
    "            pred_sigmoid=torch.sigmoid(pred)\n",
    "        \n",
    "            pred_round=model.round_prediction(pred_sigmoid.detach().numpy())            \n",
    "            \n",
    "            \n",
    "            for cnt,iid in enumerate(batch_id):\n",
    "                lline=\"{},{}\\n\".format(iid,pred_round[cnt])            \n",
    "                ffile.write(lline)                            \n",
    "\n",
    "    ffile.close()\n",
    "    \n",
    "    print(\"DONE\")   \n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 20:37:03.666013 TEST STARTED\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "test_fn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
